{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VV8XnOA5dDB"
   },
   "source": [
    "<div align=center>\n",
    "\t\t\n",
    "<p></p>\n",
    "<p></p>\n",
    "<font size=5>\n",
    "In the Name of God\n",
    "<font/>\n",
    "<p></p>\n",
    " <br/>\n",
    "    <br/>\n",
    "    <br/>\n",
    "<font color=#FF7500>\n",
    "Sharif University of Technology - Departmenet of Computer Engineering\n",
    "</font>\n",
    "<p></p>\n",
    "<font color=blue>\n",
    "Artifical Intelligence - Dr. Mohammad Hossein Rohban\n",
    "</font>\n",
    "<br/>\n",
    "<br/>\n",
    "Fall 2021\n",
    "\n",
    "</div>\n",
    "\n",
    "<hr/>\n",
    "\t\t<div align=center>\n",
    "\t\t    <font color=red size=6>\n",
    "\t\t\t    <br />\n",
    "Practical Assignment 5-Q1\n",
    "            \t<br/>\n",
    "\t\t\t</font>\n",
    "    <br/>\n",
    "    <br/>\n",
    "<font size=4>\n",
    "\t\t\t<br/><br/>\n",
    "Deadline:  Bahman 17th\n",
    "                <br/><b>\n",
    "              Cheating is Strongly Prohibited\n",
    "                </b><br/><br/>\n",
    "                <font color=red>\n",
    "Please run all the cells.\n",
    "     </font>\n",
    "</font>\n",
    "                <br/>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xTcFrqL5dDM"
   },
   "source": [
    "# Personal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collectable": true,
    "execution": {
     "iopub.execute_input": "2021-10-01T16:01:36.762477Z",
     "iopub.status.busy": "2021-10-01T16:01:36.762155Z",
     "iopub.status.idle": "2021-10-01T16:01:36.764025Z",
     "shell.execute_reply": "2021-10-01T16:01:36.763754Z"
    },
    "id": "f3-7dVqZ5dDN"
   },
   "outputs": [],
   "source": [
    "# Set your student number\n",
    "student_number = 98102284\n",
    "Name = 'Parsa'\n",
    "Last_Name = 'Mohammadian'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bww0nnn-5dDQ"
   },
   "source": [
    "# Rules\n",
    "- You are not allowed to use provided codes that can be found on the internet. \n",
    "- If you want to use a library which is not already imported, you must ask a question on Quera to get the permission of using that.\n",
    "- Do not hesitate to ask questions on Quera, if you have any.\n",
    "- This assignment is due Bahman 17th 23:59:59. you can use up to 1 grace day for this assignment and the hard deadline is Bahman 18th 23:59:59."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WflBoXk5dDT"
   },
   "source": [
    "# Q1 (30Points)\n",
    "\n",
    "## Mountain Car\n",
    "\n",
    "The OpenAI Gym library includes a set of Python Reinforcement Learning environments. In this question, we examine the Mountain Car environment of this collection. Mountain Car is a Reinforcement Learning task that aims to learn the policy of climbing a steep hill and reaching the flag-marked goal. Also, the car engine is not powerful enough to climb straight up the hill on the right. Therefore, it must gain enough acceleration by climbing the hill on the left.\n",
    "\n",
    "\n",
    "In this case, the state of the car is determined by an array containing its position and speed.\n",
    "\n",
    "| Num | Observation | Min | Max|\n",
    "| --- | --- | --- | ---|\n",
    "| 0 | Position | -1.3 | 0.5 |\n",
    "| 1 | Velocity | -0.07 | 0.07|\n",
    "\n",
    "The intelligent agent is allowed to perform three movements of right push, no push and left push in each step. The agent's move will be given to the environment and the environment returns the next state along with the movement reward. For each step that the car does not reach the target, a cost of -1 is considered. Use Q-learning to find the optimal policy in each case. To do this, you must complete the functions below.\n",
    "\n",
    "** Note that you will receive the full score of this question only if you can achieve a score of -115 with your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwp5Lt9v5dDU"
   },
   "source": [
    "<font size=4>\n",
    "Author: Atoosa Chegini\n",
    "\t\t\t<br/>\n",
    "                <font color=red>\n",
    "Please run all the cells.\n",
    "     </font>\n",
    "</font>\n",
    "                <br/>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collectable": true,
    "id": "8bf4cK8A5dDW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BRe1Iov16eXa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.1, gamma=0.99, num_states=2, num_actions=3, num_episodes=10000, pos_discretize_size=36, vel_discretize_size=16, epsilon=0.9\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "# initialization (parameters such as learning rate, gamma, number of states, number of actions, and q_table)\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "num_episodes = int(2e4)\n",
    "pos_discretize_size = 36\n",
    "vel_discretize_size = 16\n",
    "epsilon = 0.9\n",
    "q_table = np.zeros((pos_discretize_size*vel_discretize_size, num_actions))\n",
    "print(f'{alpha=}, {gamma=}, {num_states=}, {num_actions=}, {num_episodes=}, {pos_discretize_size=}, {vel_discretize_size=}, {epsilon=}')\n",
    "# You may change the inputs of any function as you desire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cD_eGcgS6iYC"
   },
   "source": [
    "Next cell wants you supplement two functions. First for transforming the continuous space into discrete one (in order to make using q_table feasible), second for updating q_values based on the last action done by agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3b4NwIH96hwG"
   },
   "outputs": [],
   "source": [
    "# This is just one example of a discretization function. You can change it as you want.\n",
    "def discretize_state(x, minn, step):\n",
    "    return int((x - minn) / step)\n",
    "\n",
    "\n",
    "def env_state_to_Q_state(state):\n",
    "    [position, velocity] = state\n",
    "    pos_low, pos_high = -1.2, 0.6\n",
    "    vel_low, vel_high = -0.07, 0.07\n",
    "    # Complete this function!\n",
    "    pos_step = (pos_high - pos_low) / (pos_discretize_size-1)\n",
    "    vel_step = (vel_high - vel_low) / (vel_discretize_size-1)\n",
    "    idx1 = discretize_state(position, pos_low, pos_step)\n",
    "    idx2 = discretize_state(velocity, vel_low, vel_step)\n",
    "    return idx1*vel_discretize_size + idx2\n",
    "\n",
    "\n",
    "def update_q(current_state, next_state, action, reward):\n",
    "    best_future_q = np.max(q_table[next_state])\n",
    "    # Complete this function!\n",
    "    q_table[current_state][action] = q_table[current_state][action] + alpha * (reward + gamma * best_future_q - q_table[current_state][action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test desctretization function\n",
    "assert env_state_to_Q_state([-1.2, -0.07]) == 0\n",
    "assert env_state_to_Q_state([0.6, 0.07]) == pos_discretize_size*vel_discretize_size-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQopme1U6mkQ"
   },
   "source": [
    "At the following cell, the ends of two functions are getting current action based on the policy and defining the training process respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "pb46h62R6k0W"
   },
   "outputs": [],
   "source": [
    "# You may change the inputs of any function as you desire.\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def get_action(current_state):\n",
    "    return np.argmax(q_table[current_state])\n",
    "\n",
    "\n",
    "def q_learning():\n",
    "    pass\n",
    "    # Complete this funciton!\n",
    "    global epsilon\n",
    "    for e in range(1, num_episodes+1):\n",
    "        state, done = env.reset(), False\n",
    "        while not done:\n",
    "            random_action = random.randint(0, num_actions-1)\n",
    "            random_prob = random.random()\n",
    "            if random_prob < epsilon:\n",
    "                action = random_action\n",
    "            else:\n",
    "                action = get_action(env_state_to_Q_state(state))\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            q_state = env_state_to_Q_state(state)\n",
    "            next_q_state = env_state_to_Q_state(next_state)\n",
    "            update_q(q_state, next_q_state, action, reward)\n",
    "            state = next_state\n",
    "        epsilon *= 0.99\n",
    "        clear_output(wait=True)\n",
    "        print(f'Episode {e}/{num_episodes} finished.')\n",
    "\n",
    "\n",
    "def save_policy():\n",
    "    policy = []\n",
    "    for item in q_table:\n",
    "        policy.append(np.argmax(item))\n",
    "    np.save('policy.npy', policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "S10pZP4T6tcw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10000/10000 finished.\n"
     ]
    }
   ],
   "source": [
    "q_learning()\n",
    "save_policy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_policy()\n",
    "np.load(\"policy.npy\")\n",
    "assert not np.all(np.load(\"policy.npy\") == 0)\n",
    "np.count_nonzero(np.load(\"policy.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kOR1PMec7ube"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 0 2 2 2 1 0 0 0 0 0 0 0 1 2 1 0 0 1 2 2 2 2 0 0 0 0 0 0 0 2 0 0\n",
      " 2 2 2 2 2 1 0 0 0 0 0 0 0 0 0 1 1 2 2 2 1 2 2 0 0 0 0 0 0 2 2 1 0 0 2 2 2\n",
      " 2 2 0 0 0 0 2 0 1 1 0 0 2 1 2 2 2 2 0 0 0 0 2 0 1 1 0 0 2 2 1 0 2 2 2 0 0\n",
      " 0 1 0 0 0 2 0 0 2 1 0 2 2 2 0 0 0 0 2 0 0 0 0 0 2 2 2 1 1 2 0 0 0 0 1 0 0\n",
      " 0 0 2 2 2 1 0 2 2 1 0 0 0 0 0 0 0 0 2 2 2 2 1 2 1 1 0 0 1 0 0 0 0 0 0 0 2\n",
      " 1 0 2 2 2 0 0 0 0 1 0 0 0 0 0 2 1 2 0 2 1 0 0 2 1 1 1 0 1 0 0 2 2 0 2 2 2\n",
      " 0 0 2 2 1 0 0 0 0 0 2 2 0 2 2 2 0 0 0 2 0 0 0 0 0 0 2 2 1 2 2 2 0 0 1 0 0\n",
      " 1 1 0 0 0 2 2 0 1 2 2 0 0 1 2 1 0 0 0 1 0 2 0 1 0 2 2 0 0 0 1 1 0 1 0 0 0\n",
      " 2 2 1 0 0 2 0 0 0 1 0 2 0 0 0 1 0 0 1 2 2 2 0 0 0 0 0 2 2 0 0 0 1 1 0 2 2\n",
      " 2 0 0 0 2 0 1 2 0 0 0 1 1 1 2 2 2 0 0 0 0 0 2 1 0 1 2 0 0 1 2 0 2 0 0 0 0\n",
      " 0 0 2 1 2 1 0 0 0 2 2 1 0 0 0 0 1 0 0 1 2 0 1 1 2 1 2 0 0 0 0 0 0 1 0 0 0\n",
      " 0 1 2 2 1 0 0 0 0 0 0 0 2 0 1 2 0 2 2 2 2 2 0 0 0 0 0 0 0 1 0 1 1 2 2 2 2\n",
      " 2 0 0 0 0 0 0 0 2 0 0 0 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 2 1 2 2 2 0 0 0 0\n",
      " 0 0 0 0 1 2 0 2 2 1 2 2 0 0 0 0 0 0 0 0 0 2 0 2 1 2 2 2 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 2 2 0 0 0 0 0 0 0 0 0 0 0 2 1 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Average score over 1000 run : -144.885\n"
     ]
    }
   ],
   "source": [
    "# Attention: don't change this function. we will use this to grade your policy which you will hand in with policy.npy\n",
    "# btw you can use it to see how you are performing. Uncomment two lines which are commented to be able to see what is happening visually.\n",
    "def score():\n",
    "    policy, scores = np.load(\"policy.npy\"), []\n",
    "    print(policy)\n",
    "    for episode in range(1000):\n",
    "        # print(f\"******Episode {episode}\")\n",
    "        state, score, done, step = env_state_to_Q_state(\n",
    "            env.reset()), 0, False, 0\n",
    "        while not done:\n",
    "            # time.sleep(0.04)\n",
    "            action = policy[state]\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            state = env_state_to_Q_state(state)\n",
    "            step += 1\n",
    "            score += int(reward)\n",
    "            # env.render()\n",
    "        # print(f\"Score:{score}\")\n",
    "        scores.append(score)\n",
    "    print(f\"Average score over 1000 run : {np.array(scores).mean()}\")\n",
    "\n",
    "\n",
    "score()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Q_learning.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "f336af7d54ba0f0c1daaf2256eb85f31e983e88153daf7a27ef3ea6c724faba4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
